import pandas as pd
import streamlit as st
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi

# --- Page Config ---
st.set_page_config(page_title="YouTube Channel Analyzer")

# --- Helpers ---
@st.cache_data
def search_videos_global(keyword, max_results=50):
    res = YOUTUBE.search().list(
        part="snippet",
        q=keyword,
        type="video",
        maxResults=max_results
    ).execute()
    return [item["id"]["videoId"] for item in res["items"]]

@st.cache_data
def search_videos_in_channel(channel_id, keyword, max_results=50):
    res = YOUTUBE.search().list(
        part="snippet",
        channelId=channel_id,
        q=keyword,
        type="video",
        maxResults=max_results
    ).execute()
    return [item["id"]["videoId"] for item in res["items"]]

@st.cache_data
def fetch_video_list(channel_id):
    uploads_pl = YOUTUBE.channels().list(
        part="contentDetails", id=channel_id
    ).execute()["items"][0]["contentDetails"]["relatedPlaylists"]["uploads"]
    video_ids, next_page = [], None
    while True:
        pl = YOUTUBE.playlistItems().list(
            part="snippet", playlistId=uploads_pl,
            maxResults=50, pageToken=next_page
        ).execute()
        video_ids += [i["snippet"]["resourceId"]["videoId"] for i in pl["items"]]
        next_page = pl.get("nextPageToken")
        if not next_page:
            break
    return video_ids

@st.cache_data
def fetch_video_details(video_ids):
    rows = []
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        res = YOUTUBE.videos().list(
            part="snippet,statistics",
            id=",".join(batch)
        ).execute()
        for item in res["items"]:
            rows.append({
                "id": item["id"],
                "title": item["snippet"]["title"],
                "thumbnail": f"https://img.youtube.com/vi/{item['id']}/mqdefault.jpg",
                "views": int(item["statistics"].get("viewCount", 0))
            })
    return pd.DataFrame(rows)

# --- UI & Main ---
st.title("YouTube Channel Analyzer")

key         = st.text_input("ğŸ”‘ YouTube API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
channel_url = st.text_input("ğŸ”— (ì„ íƒ) ë¶„ì„í•  ì±„ë„ URL")
keyword     = st.text_input("ğŸ” (ì„ íƒ) ê²€ìƒ‰ í‚¤ì›Œë“œ")
use_search  = st.checkbox("í‚¤ì›Œë“œ ê¸°ë°˜ ê¸€ë¡œë²Œ ê²€ìƒ‰", help="ì²´ë„ ë¶„ì„ì´ ì•„ë‹ˆë¼ YouTube ì „ì²´ì—ì„œ ê²€ìƒ‰")

if key:
    YOUTUBE = build("youtube", "v3", developerKey=key)

    # êµ¬ë…ì ìˆ˜ (ì±„ë„ URLì´ ìˆì„ ë•Œë§Œ)
    if channel_url:
        # extract channel_id (ê°„ë‹¨íˆ URL ë’¤ë¶€ë¶„)
        channel_id = channel_url.split("?")[0].split("/")[-1]
        sub_info = YOUTUBE.channels().list(part="statistics", id=channel_id).execute()
        sub_count = int(sub_info["items"][0]["statistics"].get("subscriberCount", 0))
        st.write(f"**êµ¬ë…ì ìˆ˜:** {sub_count:,}")

    # ì˜ìƒ ID ê°€ì ¸ì˜¤ê¸°
    with st.spinner("ì˜ìƒ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„ ì¤‘..."):
        if use_search and keyword:
            if channel_url:
                # ì±„ë„ ë‚´ ê²€ìƒ‰
                ids = search_videos_in_channel(channel_id, keyword)
            else:
                # ì „ì²´ ê²€ìƒ‰
                ids = search_videos_global(keyword)
        elif channel_url:
            # ì±„ë„ ì „ì²´ ì—…ë¡œë“œ
            ids = fetch_video_list(channel_id)
        else:
            st.info("ì±„ë„ URL ë˜ëŠ” ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            st.stop()

        df = fetch_video_details(ids)

    # í‰ê·  ì •ì˜ ê¸°ì¤€
    # â€” ì±„ë„ ë¶„ì„ ì‹œì—” ì±„ë„ ì „ì²´ í‰ê· , ê¸€ë¡œë²Œ ê²€ìƒ‰ ì‹œì—” ê²€ìƒ‰ ê²°ê³¼ í‰ê· 
    if channel_url and not use_search:
        full_ids = fetch_video_list(channel_id)
        avg = fetch_video_details(full_ids)["views"].mean()
    else:
        avg = df["views"].mean() if not df.empty else 0
    st.write(f"**í‰ê·  ì¡°íšŒìˆ˜ ê¸°ì¤€:** {avg:,.0f}")

    # ë“±ê¸‰ ë§¤ê¸°ê¸°
    def grade(v):
        if v == 0:        return "0"
        if avg == 0:      return "BAD"
        if v >= 1.5*avg:  return "GREAT"
        if v >= avg:      return "GOOD"
        return "BAD"

    df["label"] = df["views"].apply(grade)

    # ì •ë ¬ ì˜µì…˜
    sort = st.selectbox("ì •ë ¬ ê¸°ì¤€", ["ì¡°íšŒìˆ˜ ë‚´ë¦¼ì°¨ìˆœ", "ë“±ê¸‰ë³„ ì •ë ¬"])
    if sort == "ì¡°íšŒìˆ˜ ë‚´ë¦¼ì°¨ìˆœ":
        df = df.sort_values("views", ascending=False)
    else:
        order = {"GREAT":0, "GOOD":1, "BAD":2, "0":3}
        df = df.sort_values(by="label", key=lambda c: c.map(order))

    # ê²°ê³¼ ì¶œë ¥
    for i, row in df.iterrows():
        c1, c2, c3 = st.columns([1,3,1])
        c1.image(row["thumbnail"], width=120)
        c2.markdown(f"**{row['title']}**  \nì¡°íšŒìˆ˜: {row['views']:,}")
        color = {"GREAT":"#CCFF00","GOOD":"#00AA00","BAD":"#DD0000","0":"#888888"}[row["label"]]
        c3.markdown(f"<span style='color:{color}; font-weight:bold'>{row['label']}</span>",
                    unsafe_allow_html=True)
        if c3.button("ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´", key=f"t{i}"):
            try:
                segs = YouTubeTranscriptApi.get_transcript(row["id"])
                text = "\n".join(s["text"] for s in segs)
                st.download_button("TXT ì €ì¥", text, file_name=f"{row['id']}.txt")
            except Exception as e:
                st.error(f"ìŠ¤í¬ë¦½íŠ¸ ì˜¤ë¥˜: {e}")




