import pandas as pd
import streamlit as st
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi

# --- Page Config ---
st.set_page_config(page_title="YouTube Channel Analyzer")

# --- Helpers ---
@st.cache_data
def search_videos_global(keyword, max_results, region_code, duration):
    res = YOUTUBE.search().list(
        part="snippet",
        q=keyword,
        type="video",
        maxResults=max_results,
        regionCode=region_code,
        videoDuration=duration
    ).execute()
    return [item["id"]["videoId"] for item in res["items"]]

@st.cache_data
def search_videos_in_channel(channel_id, keyword, max_results, region_code, duration):
    res = YOUTUBE.search().list(
        part="snippet",
        channelId=channel_id,
        q=keyword,
        type="video",
        maxResults=max_results,
        regionCode=region_code,
        videoDuration=duration
    ).execute()
    return [item["id"]["videoId"] for item in res["items"]]

@st.cache_data
def fetch_video_list(channel_id):
    uploads_pl = YOUTUBE.channels().list(
        part="contentDetails", id=channel_id
    ).execute()["items"][0]["contentDetails"]["relatedPlaylists"]["uploads"]
    vids, next_page = [], None
    while True:
        pl = YOUTUBE.playlistItems().list(
            part="snippet", playlistId=uploads_pl,
            maxResults=50, pageToken=next_page
        ).execute()
        vids += [i["snippet"]["resourceId"]["videoId"] for i in pl["items"]]
        next_page = pl.get("nextPageToken")
        if not next_page:
            break
    return vids

@st.cache_data
def fetch_video_details(video_ids):
    rows = []
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        res = YOUTUBE.videos().list(
            part="snippet,statistics",
            id=",".join(batch)
        ).execute()
        for item in res["items"]:
            rows.append({
                "id": item["id"],
                "title": item["snippet"]["title"],
                "thumbnail": f"https://img.youtube.com/vi/{item['id']}/mqdefault.jpg",
                "views": int(item["statistics"].get("viewCount", 0))
            })
    return pd.DataFrame(rows)

# --- UI & Main ---
st.title("YouTube Channel Analyzer")

# ì…ë ¥
key = st.text_input("ğŸ”‘ YouTube API í‚¤", type="password")
channel_url = st.text_input("ğŸ”— ì±„ë„ URL (ì„ íƒ)")
keyword = st.text_input("ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ (ì„ íƒ)")
use_search = st.checkbox("ğŸ” í‚¤ì›Œë“œ ê²€ìƒ‰ ëª¨ë“œ")

# ê²€ìƒ‰ ì˜µì…˜
col1, col2, col3 = st.columns(3)
with col1:
    region = st.selectbox("ê²€ìƒ‰ êµ­ê°€", ["KR", "US", "JP"], format_func=lambda x: {"KR":"í•œêµ­", "US":"ë¯¸êµ­", "JP":"ì¼ë³¸"}[x])
with col2:
    max_res = st.selectbox("ê²€ìƒ‰ ê°œìˆ˜", [50, 100, 200])
with col3:
    dur = st.selectbox("ì˜ìƒ ìœ í˜•", ["any", "short", "long"], format_func=lambda x: {"any":"ì „ì²´", "short":"ì‡¼ì¸ ", "long":"ë¡±í¼"}[x])

if key:
    YOUTUBE = build("youtube", "v3", developerKey=key)
    channel_id = None
    # ì±„ë„ ID ì¶”ì¶œ
    if channel_url:
        channel_id = channel_url.split("?")[0].split("/")[-1]
        info = YOUTUBE.channels().list(part="statistics", id=channel_id).execute()["items"][0]["statistics"]
        st.write(f"**êµ¬ë…ì ìˆ˜:** {int(info.get('subscriberCount',0)):,}")

    # ì˜ìƒ ID ì·¨í•©
    if use_search and keyword:
        if channel_id:
            vids = search_videos_in_channel(channel_id, keyword, max_res, region, dur)
        else:
            vids = search_videos_global(keyword, max_res, region, dur)
    elif channel_id:
        vids = fetch_video_list(channel_id)
    else:
        st.info("ì±„ë„ URLì´ë‚˜ í‚¤ì›Œë“œ ê²€ìƒ‰ ëª¨ë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        st.stop()

    # ì„¸ë¶€ ì •ë³´ ì¡°íšŒ
    df = fetch_video_details(vids)

    # í‰ê·  ì¡°íšŒìˆ˜ ê¸°ì¤€
    if channel_id and not (use_search and keyword):
        avg = fetch_video_details(fetch_video_list(channel_id))["views"].mean()
    else:
        avg = df["views"].mean() if not df.empty else 0
    st.write(f"**í‰ê·  ì¡°íšŒìˆ˜:** {avg:,.0f}")

    # ë“±ê¸‰
    def grade(v):
        if v==0: return "0"
        if avg==0: return "BAD"
        if v>=1.5*avg: return "GREAT"
        if v>=avg: return "GOOD"
        return "BAD"
    df["label"] = df["views"].apply(grade)

    # ì •ë ¬
    order_map = {"GREAT":0, "GOOD":1, "BAD":2, "0":3}
    sort = st.selectbox("ì •ë ¬", ["ì¡°íšŒìˆ˜ ë‚´ë¦¼ì°¨ìˆœ", "ë“±ê¸‰ë³„"])
    if sort=="ì¡°íšŒìˆ˜ ë‚´ë¦¼ì°¨ìˆœ": df=df.sort_values("views", ascending=False)
    else: df=df.sort_values(by="label", key=lambda c: c.map(order_map))

    # ì¶œë ¥
    for i,row in df.iterrows():
        c1,c2,c3=st.columns([1,3,1])
        c1.image(row["thumbnail"], width=120)
        c2.markdown(f"**{row['title']}**  \nì¡°íšŒìˆ˜: {row['views']:,}")
        color={'GREAT':'#CCFF00','GOOD':'#00AA00','BAD':'#DD0000','0':'#888888'}[row['label']]
        c3.markdown(f"<span style='color:{color}; font-weight:bold'>{row['label']}</span>", unsafe_allow_html=True)
        if c3.button("ìŠ¤í¬ë¦½íŠ¸ ë‹¤ìš´", key=f"t{i}"):
            try:
                segs = YouTubeTranscriptApi.get_transcript(row['id'])
                txt = "\n".join(s['text'] for s in segs)
                st.download_button("ë‹¤ìš´ë¡œë“œ", txt, file_name=f"{row['id']}.txt")
            except Exception as e:
                st.error(f"ìŠ¤í¬ë¦½íŠ¸ ì˜¤ë¥˜: {e}")




